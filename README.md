# NAI-NLP-Project
NLP Project

**#Autorzy:**
1. Filip Bianga (s19329)
2. Dorota Matkowska (s19637)
3. Daniek Matkowski (s18117)
4. Adam Tomporowski (s16740)

**#Milestone 01**		
1. Załóż repozytorium wraz z system do zarządzania zadaniami (np. GitLab i Issue).							
2. Zaplanuj zadania przypisując je do poszczególnych członków N-osobowego zespołu.							
3. Przygotuj listę hobby człownków zespołu (dodaj do read.me projektu); każdy członek zespołu musi mieć unikatowe zainteresowania						
4. Przeprowadź research zagadnienia w oparciu o źródła w ilości nie mniejszej niż 5 x N ;							
5. Sporządź podsumowanie analizy źródeł; udostępnij repozytorium; termin 25.12.2021;

**#Milestone 02**				
1. Zaimplementuj czatbota, który komunikuje się z użytkownikiem w języku natuarlnym.				
2. Czatbot komunikuje się z z użytkonikiem po polsku jak i angielsku.				
3. Czatbot jest w stanie rozmawiać o hobby członków zespołu.				
4. Przygotuj prezentację o zrealizowanych zadaniach; 				
5. Prezentacja rozwiązania 15.01.2022			


**#Hobby**
Filip: 
Dorota:
Daniel:
Adam:


**#Analiza źródeł - podsumowanie:**
1. Filip
2. Dorota:

**a) https://www.youtube.com/watch?v=IUbFMt_4_Hw**

Pierwszym wynikiem po wpisaniu w  wyszkiwarce Google wyrażenia "NLP" jest ,,programowanie neurolingwistyczne". Bez wchodzenia w szczegóły prawdopodobnie chodzi o coś co ma związek z  wpływaniem na swój umysł, na czyiś umysł itd. 

Jednak nas interesuje NLP w kontekście sztucznej inteligencji, gdzie rozwija się jako Natural Language Processing czyli przetwarzanie języka anturalnego, odpowiada na pytanie "Jak nauczyć komputer posługiwac się pojeciami, którymi na codzień posługują się ludzie?".

Pierwsze z zastosowań NLP  to rozpoznawanie mowy; mówimy do mikrofonu lub posiadamy nagranie, chcemy żeby program odpowiedział co zostało nagrane. 

Kolejnym z zastosowaqń jest modelowanie języka, wykorzystywane często w smartfonach - program podpowiada najbardziej prawdopoodbne kolejne słowo pasujące do już podanej sekwencji słów. 

NPL wykorzystywane jest również w tłumaczeniu maszynowym (tłumacznie zdania z języka A na język B), analizie sentymentu(określenie wydźwięku zdania: czy pozytywny czy negatywny, neutralny), automatycznej sumaryzacji (streszczanie długiego tekstu do kilku zdań), generowaniu języka naturalnego(wygeneruj tekst wyglądający jak napisany przez człowieka), question answering (systemy odpowiadające na pytania), czatbotach (nieustrukturyzowane konwersacje, rozrywka, terapia, obsługa klienta) oraz w systemach dialogowych(dialog zorientowany na wykonanie zadania np. asystenty głosowe). 

NLP jest trudne dla komputera poprzez zjawiska takie jak: polisemia (wieloznacznosć), ironia, mowa:homofonia, kontekst, specyfika danego języka, najważniejszy problem to niedobór danych w danym języku (większość jest po angielsku);  

w 1950 roku Alan Turing zaczął sie zastanawiać co to znaczy, ze maszyna jest inteligenta, czy można mówić o inteligentnych maszynach  a jeśli tak to w jaki sposób? Zaproponował coś co dzisiaj znamy jako test Turinga. Jest dwóch uczestników dialogu : człowiek i maszyna oraz niezależny sędzia, który też jest człowiekiem i stara sie odróżnić kto jest maszyną a kto człowiekiem. Jeśli nie jest w stanie odróżnić to można powiedzieć, ze test Turinga jest zaliczony i że maszyna jest inteligenta.

W 1972 powstał czatbot o nazwie Parry - twórca zaproponował, żeby sędziami byli psychiatrzy a czatbot dostał osobowość osoby chorej na schizofrenie i wszelkie pomyłki i  brak podążania za dialogiem sędzia mógł sklasyfikować jako skutek choroby psychicznej - podobno połowa psychiatrów nabrała się, że jest to człowiek. Ze względu na naciąganie osobowości nie uznaje się tego jako przejscie testu Turinga

W 1996 roku pojawił się czatbot Eliza - udający psychoterapeutę, czatbot oparty na prostych regułach, reagował na pewne słowa kluczowe dając generyczne odpowiedzi.

W 2014 roku czatbot o nazwie Eugene Goostman podszywał się pod 13 letniego chłopca z Ukrainy, jego wszelkie braki w języku angielskim czy ogólna nieznajomosć świata mogła być wyjaśniona jego osobowością.

W pewnym momencie stwierdzono, ze oparte na regułach systemy donikąd nie prowadzą i jeśli chcemy rozwijać AI trzeba się skupić na czymś co zostało nazwane statystycznym NLP: (~1980- 2010) - stworzono duże zbiory danych, stosując klasyczne elementy uczenia maszynowego, duże zespoły projektowe złożone zarówno z infromatyków jak i ekspertów językowych mogły rozwiązywać problemy NLP z użyciem konkretnego modelu, zboru danych. 

Od 2011 roku zaczęły się na rynku pojawiać systenty głosowe. Pierwsza była Siri(Apple). W kolejnych latach (Cortana), Amazon (Alexa), Google (Google Asistant) czy Samsung(Bixby) wprowadziły na rynek swoich asystentów. 

Po statysztycznym NLP nadeszła rewolucja a z nią "Deep learning tsunami". W 2012 roku po raz pierwszy zastosowano głęboką sieć neuronową do klasyfikacji obrazu. 

Furorę zrobiły rekurencyjne sieci neuronowe służące do modelowania sekwencji słów w kolejncyh chwilach czasu. W takiej sieci wejściem jest słowo, coś się dzieje w srodku i dostajemy wyjście. Przy czym wyjście w sieci rekurencyjnej zależy nie tylko od wejścia w danej chwili ale także od stanu sieci w chwili poprzedniej. Ten koncept był w teorii znany w latach 80 XX wieku ale swoją pierwszą młodość przeżywał w roku 2014/2015 roku.

W 2017 roku chciano zrezygnować z sieci rekurencyjnych i zaproponowano mechaniznm atencji (sieć Tranformer) - stwierdzono, że równie dobrze jak nie lepiej można rozwiązywać zadania NLP . 

W 2018 roku natomiast zaproponowano GPT-2 model języka, oparty na architekturze głęgokiej sieci Transformer, wygenerowany na dużej ilości tekstu. 


4. Daniel 
5. Adam
							


