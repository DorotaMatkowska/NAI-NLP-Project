# NAI-NLP-Project
NLP Project

**#Autorzy:**
1. Filip Bianga (s19329)
2. Dorota Matkowska (s19637)
3. Daniek Matkowski (s18117)
4. Adam Tomporowski (s16740)

**#Milestone 01**		
1. Załóż repozytorium wraz z system do zarządzania zadaniami (np. GitLab i Issue).							
2. Zaplanuj zadania przypisując je do poszczególnych członków N-osobowego zespołu.							
3. Przygotuj listę hobby człownków zespołu (dodaj do read.me projektu); każdy członek zespołu musi mieć unikatowe zainteresowania						
4. Przeprowadź research zagadnienia w oparciu o źródła w ilości nie mniejszej niż 5 x N ;							
5. Sporządź podsumowanie analizy źródeł; udostępnij repozytorium; termin 25.12.2021;

**#Milestone 02**				
1. Zaimplementuj czatbota, który komunikuje się z użytkownikiem w języku natuarlnym.				
2. Czatbot komunikuje się z z użytkonikiem po polsku jak i angielsku.				
3. Czatbot jest w stanie rozmawiać o hobby członków zespołu.				
4. Przygotuj prezentację o zrealizowanych zadaniach; 				
5. Prezentacja rozwiązania 15.01.2022			


**#Hobby**
Filip: Muzyka
Dorota:
Daniel:
Adam:


**#Analiza źródeł - podsumowanie:**
1. Filip:

**a) https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1**

NLP reprezentuje automatyczną obsługę naturalnego języka ludzkiego, takiego jak mowa lub tekst, i chociaż sama koncepcja jest fascynująca, prawdziwa wartość tej technologii pochodzi z przypadków użycia.

Kilka przykładów użycia:

1. NLP umożliwia rozpoznawanie i przewidywanie chorób na podstawie elektronicznej dokumentacji medycznej. Ta zdolność jest badana w schorzeniach, od chorób sercowo-naczyniowych po depresję, a nawet schizofrenię. Na przykład Amazon Comprehend Medical to usługa, która wykorzystuje NLP do wyodrębniania stanów chorobowych , leków i wyników leczenia z notatek pacjentów, raportów z badań klinicznych i innych elektronicznych kart zdrowia.

2. Organizacje mogą określić, co klienci mówią o usłudze lub produkcie, identyfikując i wydobywając informacje ze źródeł takich jak media społecznościowe. Ta analiza sentymentu może dostarczyć wielu informacji o wyborach klientów i ich motywach decyzyjnych.

3. Firmy takie jak Yahoo i Google filtrują i klasyfikują Twoje e-maile za pomocą NLP, analizując tekst w e-mailach przepływających przez ich serwery i zatrzymując spam, zanim jeszcze dotrą do Twojej skrzynki odbiorczej.

4. Alexa firmy Amazon i Siri firmy Apple to przykłady inteligentnych interfejsów sterowanych głosem, które wykorzystują NLP do reagowania na komunikaty głosowe i robią wszystko, na przykład znalezienie konkretnego sklepu, poinformowanie nas o prognozie pogody, zasugerowanie najlepszej drogi do biura lub włączenie światła w domu.

**b) https://data-flair.training/blogs/python-chatbot-project/**

Chatbot to inteligentne oprogramowanie, które jest w stanie komunikować się i wykonywać działania podobne do człowieka. Chatboty są często używane w interakcji z klientem, w marketingu, w serwisach społecznościowych i natychmiastowej komunikacji z klientem. Istnieją dwa podstawowe typy modeli chatbotów w zależności od tego, jak są zbudowane; Modele oparte na pobieraniu i generatywne.

1. Modele oparte na pobieraniu
Chatbot oparty na pobieraniu wykorzystuje predefiniowane wzorce wprowadzania i odpowiedzi. Następnie używa pewnego rodzaju podejścia heurystycznego, aby wybrać odpowiednią odpowiedź. Jest szeroko stosowany w branży do tworzenia chatbotów zorientowanych na cel, w których możemy dostosować ton i przepływ chatbota, aby zapewnić naszym klientom najlepsze wrażenia.

2. Modele generatywne
Modele generatywne nie są oparte na niektórych predefiniowanych odpowiedziach.
Oparte są na sieciach neuronowych seq 2 seq. To ten sam pomysł, co tłumaczenie maszynowe. W tłumaczeniu maszynowym tłumaczymy kod źródłowy z jednego języka na inny, ale tutaj zamierzamy przekształcić dane wejściowe w dane wyjściowe. Potrzebuje dużej ilości danych i opiera się na sieciach Deep Neural.

**c) https://www.analyticsvidhya.com/blog/2021/10/complete-guide-to-build-your-ai-chatbot-with-nlp-in-python/**

Chatboty to nic innego jak aplikacje wykorzystywane przez firmy lub inne podmioty do prowadzenia automatycznej rozmowy między człowiekiem a sztuczną inteligencją. Te rozmowy mogą odbywać się za pomocą tekstu lub mowy. Chatboty muszą rozumieć i naśladować ludzką rozmowę podczas interakcji z ludźmi z całego świata. Od pierwszego chatbota, który powstał, ELIZA, do dzisiejszej ALEXA firmy Amazon, chatboty przeszły długą drogę. 


2. Dorota:

**a) https://www.youtube.com/watch?v=IUbFMt_4_Hw**

Pierwszym wynikiem po wpisaniu w  wyszkiwarce Google wyrażenia "NLP" jest ,,programowanie neurolingwistyczne". Bez wchodzenia w szczegóły prawdopodobnie chodzi o coś co ma związek z  wpływaniem na swój umysł, na czyiś umysł itd. 

Jednak nas interesuje NLP w kontekście sztucznej inteligencji, gdzie rozwija się jako Natural Language Processing czyli przetwarzanie języka naturalnego, odpowiada na pytanie "Jak nauczyć komputer posługiwac się pojeciami, którymi na codzień posługują się ludzie?".

Pierwsze z zastosowań NLP  to rozpoznawanie mowy; mówimy do mikrofonu lub posiadamy nagranie, chcemy żeby program odpowiedział co zostało nagrane. 

Kolejnym z zastosowaqń jest modelowanie języka, wykorzystywane często w smartfonach - program podpowiada najbardziej prawdopoodbne kolejne słowo pasujące do już podanej sekwencji słów. 

NPL wykorzystywane jest również w tłumaczeniu maszynowym (tłumacznie zdania z języka A na język B), analizie sentymentu(określenie wydźwięku zdania: czy pozytywny czy negatywny, neutralny), automatycznej sumaryzacji (streszczanie długiego tekstu do kilku zdań), generowaniu języka naturalnego(wygeneruj tekst wyglądający jak napisany przez człowieka), question answering (systemy odpowiadające na pytania), czatbotach (nieustrukturyzowane konwersacje, rozrywka, terapia, obsługa klienta) oraz w systemach dialogowych(dialog zorientowany na wykonanie zadania np. asystenty głosowe). 

NLP jest trudne dla komputera poprzez zjawiska takie jak: polisemia (wieloznacznosć), ironia, mowa:homofonia, kontekst, specyfika danego języka, najważniejszy problem to niedobór danych w danym języku (większość jest po angielsku);  

w 1950 roku Alan Turing zaczął sie zastanawiać co to znaczy, ze maszyna jest inteligenta, czy można mówić o inteligentnych maszynach  a jeśli tak to w jaki sposób? Zaproponował coś co dzisiaj znamy jako test Turinga. Jest dwóch uczestników dialogu : człowiek i maszyna oraz niezależny sędzia, który też jest człowiekiem i stara sie odróżnić kto jest maszyną a kto człowiekiem. Jeśli nie jest w stanie odróżnić to można powiedzieć, ze test Turinga jest zaliczony i że maszyna jest inteligenta.

W 1972 powstał czatbot o nazwie Parry - twórca zaproponował, żeby sędziami byli psychiatrzy a czatbot dostał osobowość osoby chorej na schizofrenie i wszelkie pomyłki i  brak podążania za dialogiem sędzia mógł sklasyfikować jako skutek choroby psychicznej - podobno połowa psychiatrów nabrała się, że jest to człowiek. Ze względu na naciąganie osobowości nie uznaje się tego jako przejscie testu Turinga

W 1996 roku pojawił się czatbot Eliza - udający psychoterapeutę, czatbot oparty na prostych regułach, reagował na pewne słowa kluczowe dając generyczne odpowiedzi.

W 2014 roku czatbot o nazwie Eugene Goostman podszywał się pod 13 letniego chłopca z Ukrainy, jego wszelkie braki w języku angielskim czy ogólna nieznajomosć świata mogła być wyjaśniona jego osobowością.

W pewnym momencie stwierdzono, ze oparte na regułach systemy donikąd nie prowadzą i jeśli chcemy rozwijać AI trzeba się skupić na czymś co zostało nazwane statystycznym NLP: (~1980- 2010) - stworzono duże zbiory danych, stosując klasyczne elementy uczenia maszynowego, duże zespoły projektowe złożone zarówno z infromatyków jak i ekspertów językowych mogły rozwiązywać problemy NLP z użyciem konkretnego modelu, zboru danych. 

Od 2011 roku zaczęły się na rynku pojawiać systenty głosowe. Pierwsza była Siri(Apple). W kolejnych latach (Cortana), Amazon (Alexa), Google (Google Asistant) czy Samsung(Bixby) wprowadziły na rynek swoich asystentów. 

Po statysztycznym NLP nadeszła rewolucja a z nią "Deep learning tsunami". W 2012 roku po raz pierwszy zastosowano głęboką sieć neuronową do klasyfikacji obrazu. 

Furorę zrobiły rekurencyjne sieci neuronowe służące do modelowania sekwencji słów w kolejncyh chwilach czasu. W takiej sieci wejściem jest słowo, coś się dzieje w srodku i dostajemy wyjście. Przy czym wyjście w sieci rekurencyjnej zależy nie tylko od wejścia w danej chwili ale także od stanu sieci w chwili poprzedniej. Ten koncept był w teorii znany w latach 80 XX wieku ale swoją pierwszą młodość przeżywał w roku 2014/2015 roku.

W 2017 roku chciano zrezygnować z sieci rekurencyjnych i zaproponowano mechaniznm atencji (sieć Tranformer) - stwierdzono, że równie dobrze jak nie lepiej można rozwiązywać zadania NLP . 

W 2018 roku natomiast zaproponowano GPT-2 model języka, oparty na architekturze głęgokiej sieci Transformer, wygenerowany na dużej ilości tekstu. 


**b) https://summalinguae.com/pl/technologie-jezykowe/przetwarzanie-jezyka-naturalnego-a-sztuczna-inteligencja-poznaj-5-najwazniejszych-roznic/**

NLP jest gałęzią sztucznej inteligencji. Algorytmy AI pozwalają maszynom analizować i przetwarzać ogromne ilości dostarczonych danych w krótkim czasie. Samo NLP jest dziedziną interdyscyplinarną, skupioną na języku. Łączy w sobie zagadnienia AI i językoznawstwa, co umożliwia np. automatyzację, tłumaczenie czy generowanie przez komputer tekstu zbliżonego do języka naturalnego. Za przykład można podać asystentów głosowych czy chatboty, które coraz śmielej wkraczają do obsługi klienta. 
Przetwarzanie języka naturalnego jest często utożsamiane także z technologią rozpoznawania mowy. W takich rozwiązaniach wykorzystuje ono zarówno uczenie maszynowe, jak i głębokie uczenie po to, by skutecznie pozyskiwać, przetwarzać i rozpoznawać zestawy danych, które dotyczą mowy i tekstu.


**c) https://realpython.com/nltk-nlp-python/**

Przetwarzanie języka naturalnego (NLP) jest dziedziną, która skupia się na tym, aby naturalny język ludzki był użyteczny dla programów komputerowych. NLTK, czyli Natural Language Toolkit, to pakiet Pythona, który można wykorzystać do NLP.

Warunkiem korzystania z NLTK jest posiadanie zainstalowanego Pythona. Samo NLTK instaluje sie poprzez komendę: 

$ python -m pip install nltk==3.5

Aby tworzyć wizualizacje dla rozpoznawania nazw własnych, należy zainstalować NumPy i Matplotlib:

$ python -m pip install numpy matplotlib

Tokenizacja pomaga w podzieleniu tekstu na słowa lub zdania, pozwala to na pracę z mniejszymi fragmentami tekstu. 
Jest to pierwszy krok do przekształcenia danych nieustrukturyzowanych w dane ustrukturyzowane, które są łatwiejsze do analizy.

Tokenizacja tekstu według słów pozwala na identyfikację słów, które pojawiają się szczególnie często.
 
Tokenizacja według zdań, pozwala przeanalizować, jak te słowa odnoszą się do siebie i zobaczyć więcej kontekstu. 

Ważne jest wydzielenie tzw. ,,Stop words" czyli słów, które chcemy zignorować, są to powszechne słowa, takie jak "in", "is" i "an" - często używane jako stop words, ponieważ same w sobie nie dodają wiele znaczenia do tekstu.

Słowa treściowe dostarczają informacji o tematach poruszanych w tekście lub o odczuciach autora w stosunku do tych tematów.

Słowa kontekstowe dostarczają informacji o stylu pisania.

Stemming to zadanie przetwarzania tekstu, w którym redukuje się słowa do ich rdzenia, który jest główną częścią słowa. Na przykład, słowa "pomoc" i "pomocnik" mają wspólny rdzeń "pomoc". Stemming pozwala na wyzerowanie podstawowego znaczenia słowa, a nie wszystkich szczegółów, jak to jest używane. NLTK ma więcej niż jeden stemmer.

Lematyzacja redukuje słowa do ich podstawowego znaczenia.

Podczas gdy tokenizacja pozwala na identyfikację słów i zdań, okrajanie pozwala na identyfikację fraz.

Named entities are noun phrases that refer to specific locations, people, organizations, and so on. With named entity recognition, you can find the named entities in your texts and also determine what kind of named entity they are.

Kroki i definicje opisane w tutorialu będą kluczowe przy tworzeniu chatbota.


3. Daniel 

**a) https://www.polski-chatbot.pl/natural-language-processing-nlp-czym-jest/**

To właśnie dzięki natural language processing (NLP) współczesne chatboty są sprawnymi rozmówcami. 
Dzisiejsze chatboty powstają na bazie technologii NLP i to właśnie dzięki niej potrafią zrozumieć komunikaty ze strony użytkownika oraz prawidłowo na nie zareagować.
NLP to jednak nie tylko same chatboty, ale też ich udźwiękowione wersje, czyli voiceboty oraz asystenci głosowi tacy jak Google Asystent, Alexa, Siri czy Bixby.

Chcielibyśmy, aby chatboty potrafiły podjąć się interpretacji całej złożoności ludzkiej komunikacji, jednak nawet bez tego – radzą sobie coraz lepiej. Dzisiaj bot NLP nie tylko rozumienie znaczenie słów. Chatboty potrafią świetnie zinterpretować wypowiedzi, odszyfrować ciągi alfanumeryczne, dane osobowe, rozpoznawać imiona i nazwiska, nazwy własne, adresy, numery telefonów. Dodając do tego umiejętność umieszczania dialogu w kontekście, uzyskiwane jest całkiem dobry wynik w odczytywaniu zamiarów (intencji) użytkowników. To bardzo praktyczne zastosowania, które na co dzień są bardzo przydatne w biznesie i różnego typu organizacjach. Chatboty NLP używane są bowiem nie tylko do prowadzenia prostych rozmów, ale też obsługi całych procesów.

Dostawcy usług chatbotowych zazwyczaj posiadają swoje własne NLP. Jego jakość ma duży wpływ na finalną skuteczność chatbota. Jednak nie każdy bot przygotowany na tym samym narzędziu uzyskuje identyczne wyniki. 

**b) https://sunscrapers.com/blog/8-best-python-natural-language-processing-nlp-libraries/**

Technologie oparte na NLP mogą dostarczyć szerokiego wachlarza cennych spostrzeżeń i rozwiązań problemów językowych, z jakimi mogą spotkać się konsumenci podczas interakcji z produktem.

Nie bez powodu giganci technologiczni, tacy jak Google, Amazon czy Facebook, przeznaczają miliony dolarów na tę dziedzinę badań, aby zasilić swoje chatboty, wirtualnych asystentów, silniki rekomendacji i inne rozwiązania oparte na uczeniu maszynowym.

Ponieważ NLP opiera się na zaawansowanych umiejętnościach obliczeniowych, programiści potrzebują najlepszych dostępnych narzędzi, które pomogą w pełni wykorzystać podejścia i algorytmy NLP do tworzenia usług, które mogą obsługiwać języki naturalne.

Obecnie programiści mogą korzystać z gotowych narzędzi, które upraszczają wstępne przetwarzanie tekstu, dzięki czemu mogą skupić się na budowaniu modeli uczenia maszynowego.

Python dostarcza programistom bogatą kolekcję narzędzi i bibliotek NLP, które pozwalają na obsługę dużej liczby zadań związanych z NLP, takich jak klasyfikacja dokumentów, modelowanie tematów, znakowanie części mowy (POS), wektory słów i analiza sentymentu.

Przykłady takich bibliotek:
- Natural Language Toolkit (NLTK): odegrała kluczową rolę w przełomowych badaniach nad NLP; jest podstawową biblioteką wspierającą takie zadania jak klasyfikacja, tagowanie, parsowanie, rozumowanie semantyczne i tokenizacja w Pythonie. Jest to w zasadzie główne narzędzie do przetwarzania języka naturalnego i uczenia maszynowego. 
Biblioteka ta jest dość wszechstronna, ale jest też doiść trudna w użyciu dla NLP w języku Python. NLTK może nie być dość szybki by odpowiedzieć na wymagania szybkiego użycia w produkcji. Krzywa uczenia się jest stroma, ale programiści mogą skorzystać z zasobów NLTK aby dowiedzieć się wuęcej o koncepcjach stojących za zadaniami przetwarzania języka obsługiwanymi przez ten zestaw narzędzi.

- TextBlob: jest podstawą dla programistów chcących wejść w temat NLP w Pythonie przy jednocześnie maksymalnym uzysku podczas pierwszego natknięcia się na NLTK. Zapewnia ona prosty interfejs do nauki wiekszości prostych zadań w NLP jak analiza sentymentów, post-tagging lub ekstrakcja rzeczownika z wyrażenia. Jednak ta biblioteka, podobnie jak NLTK jest zbyt powolna by sprostać wymaganiom użycia produkcyjnego.

- CoreNLP: Największą zaletą biblioteki CoreNLP jest jej szybkość i sprawność w środowiskach product dewelopmentu. Niektóre komponenty CoreNLP mogą być integrowane z NLTK co daje nam finalnie wzrost efektywności.

- Gensim: specjalizuje się w identyfikowaniu podobieństw semantycznych między dwoma dokumentami przy pomocy zestawu narzędzi do modelowania przestrzeni wektorowej oraz modelowania tematycznego.
Może obsługiwać duże bloki tekstowe za pomocą wydajnego strumieniowania danych i algorytmów przyrostowych, co jest więcej niż możemy powiedzieć o innych pakietach, które są ukierunkowane tylko na przetwarzanie wsadowe i w pamięci. Największa jego zaleta to niesamowita optymalizacja wykorzystania pamięci i szybkość przetwarzania. Zostały one osiągnięte z pomocą innej biblioteki Pythona, NumPy.

- spaCy: jest stosunkowo młodą biblioteką, która została zaprojektowana do użytku produkcyjnego. SpaCypaCy oferuje najszybszy parser składniowy dostępny obecnie na rynku. Co więcej, ponieważ zestaw narzędzi napisany jest w Cythonie, jest on również bardzo szybki i wydajny.

- polygot: oferuje szeroki zakres analiz i imponujące pokrycie językowe. Dzięki NumPy, działa również naprawdę szybko. Używanie polyglota jest podobne do spaCy - jest bardzo wydajny i prosty. Biblioteka wyróżnia się z tłumu również tym, że wymaga użycia dedykowanej komendy w linii poleceń poprzez mechanizmy pipeline.

- scikit-learn: Ta poręczna biblioteka NLP udostępnia programistom szeroki zakres algorytmów do budowy modeli uczenia maszynowego. Oferuje wiele funkcji pozwalających na wykorzystanie metody bag-of-words do tworzenia cech w celu rozwiązywania problemów klasyfikacji tekstu. Biblioteka ta nie wykorzystuje jednak sieci neuronowych do wstępnego przetwarzania tekstu. 

- Pattern: Pattern pozwala na tagowanie części mowy, analizę sentymentu, modelowanie przestrzeni wektorowej, SVM, klasteryzację, wyszukiwanie n-gramów i WordNet. Można skorzystać z parsera DOM, crawlera stron internetowych, a także kilku przydatnych API, takich jak Twitter czy Facebook. Mimo to, narzędzie to jest w zasadzie web minerem i może nie wystarczyć do wykonania innych zadań związanych z przetwarzaniem języka naturalnego.

Dzięki rozbudowanemu zestawowi narzędzi i bibliotek Python NLP programiści otrzymują wszelkie wsparcie, jakiego potrzebują podczas budowania niesamowitych narzędzi.
Te 8 bibliotek i wrodzone cechy języka Python sprawiają, że jest on najlepszym wyborem dla każdego projektu, który opiera się na maszynowym rozumieniu ludzkich języków.


4. Adam
							


