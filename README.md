# NAI-NLP-Project
NLP Project

**#Autorzy:**
1. Filip Bianga (s19329)
2. Dorota Matkowska (s19637)
3. Daniek Matkowski (s18117)
4. Adam Tomporowski (s16740)

**#Milestone 01**		
1. Załóż repozytorium wraz z system do zarządzania zadaniami (np. GitLab i Issue).							
2. Zaplanuj zadania przypisując je do poszczególnych członków N-osobowego zespołu.							
3. Przygotuj listę hobby człownków zespołu (dodaj do read.me projektu); każdy członek zespołu musi mieć unikatowe zainteresowania						
4. Przeprowadź research zagadnienia w oparciu o źródła w ilości nie mniejszej niż 5 x N ;							
5. Sporządź podsumowanie analizy źródeł; udostępnij repozytorium; termin 25.12.2021;

**#Milestone 02**				
1. Zaimplementuj czatbota, który komunikuje się z użytkownikiem w języku natuarlnym.				
2. Czatbot komunikuje się z z użytkonikiem po polsku jak i angielsku.				
3. Czatbot jest w stanie rozmawiać o hobby członków zespołu.				
4. Przygotuj prezentację o zrealizowanych zadaniach; 				
5. Prezentacja rozwiązania 15.01.2022			


**#Hobby**
Filip: Muzyka

Dorota: Filmy i Seriale

Daniel: Siłownia

Adam: MTB Enduro


**#Analiza źródeł - podsumowanie:**
1. Filip:

**a) https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1**

NLP reprezentuje automatyczną obsługę naturalnego języka ludzkiego, takiego jak mowa lub tekst, i chociaż sama koncepcja jest fascynująca, prawdziwa wartość tej technologii pochodzi z przypadków użycia.

Kilka przykładów użycia:

1. NLP umożliwia rozpoznawanie i przewidywanie chorób na podstawie elektronicznej dokumentacji medycznej. Ta zdolność jest badana w schorzeniach, od chorób sercowo-naczyniowych po depresję, a nawet schizofrenię. Na przykład Amazon Comprehend Medical to usługa, która wykorzystuje NLP do wyodrębniania stanów chorobowych , leków i wyników leczenia z notatek pacjentów, raportów z badań klinicznych i innych elektronicznych kart zdrowia.

2. Organizacje mogą określić, co klienci mówią o usłudze lub produkcie, identyfikując i wydobywając informacje ze źródeł takich jak media społecznościowe. Ta analiza sentymentu może dostarczyć wielu informacji o wyborach klientów i ich motywach decyzyjnych.

3. Firmy takie jak Yahoo i Google filtrują i klasyfikują Twoje e-maile za pomocą NLP, analizując tekst w e-mailach przepływających przez ich serwery i zatrzymując spam, zanim jeszcze dotrą do Twojej skrzynki odbiorczej.

4. Alexa firmy Amazon i Siri firmy Apple to przykłady inteligentnych interfejsów sterowanych głosem, które wykorzystują NLP do reagowania na komunikaty głosowe i robią wszystko, na przykład znalezienie konkretnego sklepu, poinformowanie nas o prognozie pogody, zasugerowanie najlepszej drogi do biura lub włączenie światła w domu.

**b) https://data-flair.training/blogs/python-chatbot-project/**

Chatbot to inteligentne oprogramowanie, które jest w stanie komunikować się i wykonywać działania podobne do człowieka. Chatboty są często używane w interakcji z klientem, w marketingu, w serwisach społecznościowych i natychmiastowej komunikacji z klientem. Istnieją dwa podstawowe typy modeli chatbotów w zależności od tego, jak są zbudowane; Modele oparte na pobieraniu i generatywne.

1. Modele oparte na pobieraniu
Chatbot oparty na pobieraniu wykorzystuje predefiniowane wzorce wprowadzania i odpowiedzi. Następnie używa pewnego rodzaju podejścia heurystycznego, aby wybrać odpowiednią odpowiedź. Jest szeroko stosowany w branży do tworzenia chatbotów zorientowanych na cel, w których możemy dostosować ton i przepływ chatbota, aby zapewnić naszym klientom najlepsze wrażenia.

2. Modele generatywne
Modele generatywne nie są oparte na niektórych predefiniowanych odpowiedziach.
Oparte są na sieciach neuronowych seq 2 seq. To ten sam pomysł, co tłumaczenie maszynowe. W tłumaczeniu maszynowym tłumaczymy kod źródłowy z jednego języka na inny, ale tutaj zamierzamy przekształcić dane wejściowe w dane wyjściowe. Potrzebuje dużej ilości danych i opiera się na sieciach Deep Neural.

**c) https://www.analyticsvidhya.com/blog/2021/10/complete-guide-to-build-your-ai-chatbot-with-nlp-in-python/**

Chatboty to nic innego jak aplikacje wykorzystywane przez firmy lub inne podmioty do prowadzenia automatycznej rozmowy między człowiekiem a sztuczną inteligencją. Te rozmowy mogą odbywać się za pomocą tekstu lub mowy. Chatboty muszą rozumieć i naśladować ludzką rozmowę podczas interakcji z ludźmi z całego świata. Od pierwszego chatbota, który powstał, ELIZA, do dzisiejszej ALEXA firmy Amazon, chatboty przeszły długą drogę. 

**d) https://www.sas.com/en_nz/insights/analytics/what-is-natural-language-processing-nlp.html**

Dlaczego NLP jest ważne?

1. Duże ilości danych tekstowych

	Przetwarzanie języka naturalnego pomaga komputerom komunikować się z ludźmi w ich własnym języku i skalować inne zadania związane z językiem. Na przykład NLP umożliwia komputerom czytanie tekstu, słuchanie mowy, interpretowanie go, mierzenie sentymentu i określanie, które części są ważne. 

	Dzisiejsze maszyny mogą analizować więcej danych opartych na języku niż ludzie, bez zmęczenia i w spójny, bezstronny sposób. Biorąc pod uwagę oszałamiającą ilość nieustrukturyzowanych danych generowanych każdego dnia, od dokumentacji medycznej po media społecznościowe, automatyzacja będzie miała kluczowe znaczenie dla pełnej analizy danych tekstowych i mowy.

2. Strukturyzacja wysoce nieustrukturyzowanego źródła danych

	Język ludzki jest zdumiewająco złożony i różnorodny. Wyrażamy się na nieskończone sposoby, zarówno werbalnie, jak i pisemnie. Istnieją nie tylko setki języków i dialektów, ale w każdym języku istnieje unikalny zestaw reguł gramatycznych i składniowych, terminów i slangu. 	Kiedy piszemy, często błędnie piszemy lub skracamy słowa lub pomijamy znaki interpunkcyjne. Kiedy mówimy, mamy regionalne akcenty, mamroczemy, jąkamy się i zapożyczamy terminy z innych języków. 

	Podczas gdy uczenie nadzorowane i nienadzorowane, a w szczególności uczenie głębokie, jest obecnie szeroko stosowane do modelowania ludzkiego języka, istnieje również potrzeba zrozumienia składniowego i semantycznego oraz wiedzy specjalistycznej w dziedzinie, które niekoniecznie są obecne w tych podejściach do uczenia maszynowego. NLP jest ważne, ponieważ pomaga rozwiązać niejasności w języku i dodaje przydatną strukturę numeryczną do danych dla wielu dalszych aplikacji, takich jak rozpoznawanie mowy lub analiza tekstu. 


Ograniczenie skarg klientów dzięki NLP:

Dla przykładu Royal Bank of Scotland wykorzystuje analizę tekstu , technikę NLP, aby wyodrębnić ważne trendy z opinii klientów w wielu formach. Firma analizuje dane z e-maili, ankiet i rozmów w call center, aby zidentyfikować podstawową przyczynę niezadowolenia klientów i wdrożyć ulepszenia.

**e) https://www.upgrad.com/blog/how-to-make-chatbot-in-python/**

W tym artykule przechodzimy krok po kroku od podstawowych pojęć czym jest chatbot aż do stworzenia własnego bota.

1. Chatbot w dzisiejszej generacji

	Obecnie dysponujemy inteligentnymi Chatbotami opartymi na sztucznej inteligencji, które wykorzystują przetwarzanie języka naturalnego (NLP), aby rozumieć polecenia człowieka (tekst i głos) i uczyć się na doświadczeniu. Chatboty stały się podstawowym narzędziem interakcji z klientami dla firm i marek, które aktywnie działają w Internecie (strona internetowa i platformy społecznościowe).

2. Biblioteka ChatterBota

	ChatterBot to biblioteka Pythona zaprojektowana do dostarczania automatycznych odpowiedzi na dane wejściowe użytkownika. Wykorzystuje kombinację algorytmów ML do generowania wielu różnych typów odpowiedzi. Ta funkcja umożliwia programistom tworzenie chatbotów za pomocą Pythona, które mogą rozmawiać z ludźmi i dostarczać odpowiednie i trafne odpowiedzi. Nie tylko to, algorytmy ML pomagają botowi poprawić jego wydajność dzięki doświadczeniu. 

Kolejną doskonałą cechą ChatterBota jest jego niezależność językowa. Biblioteka została zaprojektowana w sposób, który umożliwia wyszkolenie bota w wielu językach programowania.

3. Jak działa ChatterBot?

	Gdy użytkownik wprowadzi określone dane wejściowe w chatbocie (opracowanym na ChatterBocie), bot zapisuje je wraz z odpowiedzią do wykorzystania w przyszłości. Te dane (zgromadzone doświadczenia) pozwalają chatbotowi generować automatyczne odpowiedzi za każdym razem, gdy wprowadzane są do niego nowe dane wejściowe.

Program wybiera najbardziej pasującą odpowiedź z najbliższej instrukcji, która pasuje do danych wejściowych, a następnie dostarcza odpowiedź z już znanego wyboru stwierdzeń i odpowiedzi. Z biegiem czasu, gdy chatbot angażuje się w więcej interakcji, poprawia się dokładność odpowiedzi.

W kolejnej części artykułu przechodzimy po kolei krok po kroku jak stworzyć własnego chatbota.

2. Dorota:

**a) https://www.youtube.com/watch?v=IUbFMt_4_Hw**

Pierwszym wynikiem po wpisaniu w  wyszkiwarce Google wyrażenia "NLP" jest ,,programowanie neurolingwistyczne". Bez wchodzenia w szczegóły prawdopodobnie chodzi o coś co ma związek z  wpływaniem na swój umysł, na czyiś umysł itd. 

Jednak nas interesuje NLP w kontekście sztucznej inteligencji, gdzie rozwija się jako Natural Language Processing czyli przetwarzanie języka naturalnego, odpowiada na pytanie "Jak nauczyć komputer posługiwac się pojeciami, którymi na codzień posługują się ludzie?".

Pierwsze z zastosowań NLP  to rozpoznawanie mowy; mówimy do mikrofonu lub posiadamy nagranie, chcemy żeby program odpowiedział co zostało nagrane. 

Kolejnym z zastosowaqń jest modelowanie języka, wykorzystywane często w smartfonach - program podpowiada najbardziej prawdopoodbne kolejne słowo pasujące do już podanej sekwencji słów. 

NPL wykorzystywane jest również w tłumaczeniu maszynowym (tłumacznie zdania z języka A na język B), analizie sentymentu(określenie wydźwięku zdania: czy pozytywny czy negatywny, neutralny), automatycznej sumaryzacji (streszczanie długiego tekstu do kilku zdań), generowaniu języka naturalnego(wygeneruj tekst wyglądający jak napisany przez człowieka), question answering (systemy odpowiadające na pytania), czatbotach (nieustrukturyzowane konwersacje, rozrywka, terapia, obsługa klienta) oraz w systemach dialogowych(dialog zorientowany na wykonanie zadania np. asystenty głosowe). 

NLP jest trudne dla komputera poprzez zjawiska takie jak: polisemia (wieloznacznosć), ironia, mowa:homofonia, kontekst, specyfika danego języka, najważniejszy problem to niedobór danych w danym języku (większość jest po angielsku);  

w 1950 roku Alan Turing zaczął sie zastanawiać co to znaczy, ze maszyna jest inteligenta, czy można mówić o inteligentnych maszynach  a jeśli tak to w jaki sposób? Zaproponował coś co dzisiaj znamy jako test Turinga. Jest dwóch uczestników dialogu : człowiek i maszyna oraz niezależny sędzia, który też jest człowiekiem i stara sie odróżnić kto jest maszyną a kto człowiekiem. Jeśli nie jest w stanie odróżnić to można powiedzieć, ze test Turinga jest zaliczony i że maszyna jest inteligenta.

W 1972 powstał czatbot o nazwie Parry - twórca zaproponował, żeby sędziami byli psychiatrzy a czatbot dostał osobowość osoby chorej na schizofrenie i wszelkie pomyłki i  brak podążania za dialogiem sędzia mógł sklasyfikować jako skutek choroby psychicznej - podobno połowa psychiatrów nabrała się, że jest to człowiek. Ze względu na naciąganie osobowości nie uznaje się tego jako przejscie testu Turinga

W 1996 roku pojawił się czatbot Eliza - udający psychoterapeutę, czatbot oparty na prostych regułach, reagował na pewne słowa kluczowe dając generyczne odpowiedzi.

W 2014 roku czatbot o nazwie Eugene Goostman podszywał się pod 13 letniego chłopca z Ukrainy, jego wszelkie braki w języku angielskim czy ogólna nieznajomosć świata mogła być wyjaśniona jego osobowością.

W pewnym momencie stwierdzono, ze oparte na regułach systemy donikąd nie prowadzą i jeśli chcemy rozwijać AI trzeba się skupić na czymś co zostało nazwane statystycznym NLP: (~1980- 2010) - stworzono duże zbiory danych, stosując klasyczne elementy uczenia maszynowego, duże zespoły projektowe złożone zarówno z infromatyków jak i ekspertów językowych mogły rozwiązywać problemy NLP z użyciem konkretnego modelu, zboru danych. 

Od 2011 roku zaczęły się na rynku pojawiać systenty głosowe. Pierwsza była Siri(Apple). W kolejnych latach (Cortana), Amazon (Alexa), Google (Google Asistant) czy Samsung(Bixby) wprowadziły na rynek swoich asystentów. 

Po statysztycznym NLP nadeszła rewolucja a z nią "Deep learning tsunami". W 2012 roku po raz pierwszy zastosowano głęboką sieć neuronową do klasyfikacji obrazu. 

Furorę zrobiły rekurencyjne sieci neuronowe służące do modelowania sekwencji słów w kolejncyh chwilach czasu. W takiej sieci wejściem jest słowo, coś się dzieje w srodku i dostajemy wyjście. Przy czym wyjście w sieci rekurencyjnej zależy nie tylko od wejścia w danej chwili ale także od stanu sieci w chwili poprzedniej. Ten koncept był w teorii znany w latach 80 XX wieku ale swoją pierwszą młodość przeżywał w roku 2014/2015 roku.

W 2017 roku chciano zrezygnować z sieci rekurencyjnych i zaproponowano mechaniznm atencji (sieć Tranformer) - stwierdzono, że równie dobrze jak nie lepiej można rozwiązywać zadania NLP . 

W 2018 roku natomiast zaproponowano GPT-2 model języka, oparty na architekturze głęgokiej sieci Transformer, wygenerowany na dużej ilości tekstu. 


**b) https://summalinguae.com/pl/technologie-jezykowe/przetwarzanie-jezyka-naturalnego-a-sztuczna-inteligencja-poznaj-5-najwazniejszych-roznic/**

NLP jest gałęzią sztucznej inteligencji. Algorytmy AI pozwalają maszynom analizować i przetwarzać ogromne ilości dostarczonych danych w krótkim czasie. Samo NLP jest dziedziną interdyscyplinarną, skupioną na języku. Łączy w sobie zagadnienia AI i językoznawstwa, co umożliwia np. automatyzację, tłumaczenie czy generowanie przez komputer tekstu zbliżonego do języka naturalnego. Za przykład można podać asystentów głosowych czy chatboty, które coraz śmielej wkraczają do obsługi klienta. 
Przetwarzanie języka naturalnego jest często utożsamiane także z technologią rozpoznawania mowy. W takich rozwiązaniach wykorzystuje ono zarówno uczenie maszynowe, jak i głębokie uczenie po to, by skutecznie pozyskiwać, przetwarzać i rozpoznawać zestawy danych, które dotyczą mowy i tekstu.


**c) https://realpython.com/nltk-nlp-python/**

Przetwarzanie języka naturalnego (NLP) jest dziedziną, która skupia się na tym, aby naturalny język ludzki był użyteczny dla programów komputerowych. NLTK, czyli Natural Language Toolkit, to pakiet Pythona, który można wykorzystać do NLP.

Warunkiem korzystania z NLTK jest posiadanie zainstalowanego Pythona. Samo NLTK instaluje sie poprzez komendę: 

$ python -m pip install nltk==3.5

Aby tworzyć wizualizacje dla rozpoznawania nazw własnych, należy zainstalować NumPy i Matplotlib:

$ python -m pip install numpy matplotlib

Tokenizacja pomaga w podzieleniu tekstu na słowa lub zdania, pozwala to na pracę z mniejszymi fragmentami tekstu. 
Jest to pierwszy krok do przekształcenia danych nieustrukturyzowanych w dane ustrukturyzowane, które są łatwiejsze do analizy.

Tokenizacja tekstu według słów pozwala na identyfikację słów, które pojawiają się szczególnie często.
 
Tokenizacja według zdań, pozwala przeanalizować, jak te słowa odnoszą się do siebie i zobaczyć więcej kontekstu. 

Ważne jest wydzielenie tzw. ,,Stop words" czyli słów, które chcemy zignorować, są to powszechne słowa, takie jak "in", "is" i "an" - często używane jako stop words, ponieważ same w sobie nie dodają wiele znaczenia do tekstu.

Słowa treściowe dostarczają informacji o tematach poruszanych w tekście lub o odczuciach autora w stosunku do tych tematów.

Słowa kontekstowe dostarczają informacji o stylu pisania.

Stemming to zadanie przetwarzania tekstu, w którym redukuje się słowa do ich rdzenia, który jest główną częścią słowa. Na przykład, słowa "pomoc" i "pomocnik" mają wspólny rdzeń "pomoc". Stemming pozwala na wyzerowanie podstawowego znaczenia słowa, a nie wszystkich szczegółów, jak to jest używane. NLTK ma więcej niż jeden stemmer.

Lematyzacja redukuje słowa do ich podstawowego znaczenia.

Podczas gdy tokenizacja pozwala na identyfikację słów i zdań, okrajanie pozwala na identyfikację fraz.

Named entities are noun phrases that refer to specific locations, people, organizations, and so on. With named entity recognition, you can find the named entities in your texts and also determine what kind of named entity they are.

Kroki i definicje opisane w tutorialu będą kluczowe przy tworzeniu chatbota.


**d) https://www.pluralsight.com/guides/build-a-chatbot-with-python**

Chatbot to narzędzie oparte na sztucznej inteligencji, stworzone do konwersacji z ludźmi w ich ojczystym języku. Chatboty stały się popularne w różnych branżach i są uważane za jedno z najbardziej użytecznych zastosowań przetwarzania języka naturalnego.

NLTK jest skrótem od Natural Language Toolkit i jest wiodącą biblioteką Pythona do pracy z danymi tekstowymi. Pierwsza linia kodu poniżej importuje bibliotekę, podczas gdy druga linia używa modułu nltk.chat do importu wymaganych narzędzi.

import nltk
from nltk.chat.util import Chat, reflections

Następnie można korzystać z gotowych słowników np. Reflections lub stworzyć swój własny.

Ważnym krokiem jest stworzenie reguł, które posłużą do trenowania chatbota

Bardziej złożone reguły mogą zostać dodane, aby jeszcze bardziej wzmocnić chatbota.

**e) https://www.digitalocean.com/community/tutorials/how-to-create-an-intelligent-chatbot-in-python-using-the-spacy-nlp-library**

Tutorial pomaga stworzyć chatbota, który jest na tyle inteligentny, że potrafi odpowiedzieć na wypowiedź użytkownika - nawet jeśli użytkownik sformułuje swoją wypowiedź na różne sposoby. Chatbot używa API OpenWeather, aby uzyskać aktualną pogodę w mieście wskazanym przez użytkownika.

Zawiera szereg komend i wskazówek, które posłużą nam podczas tworzenia naszego chatbota. 



3. Daniel 

**a) https://www.polski-chatbot.pl/natural-language-processing-nlp-czym-jest/**

To właśnie dzięki natural language processing (NLP) współczesne chatboty są sprawnymi rozmówcami. 
Dzisiejsze chatboty powstają na bazie technologii NLP i to właśnie dzięki niej potrafią zrozumieć komunikaty ze strony użytkownika oraz prawidłowo na nie zareagować.
NLP to jednak nie tylko same chatboty, ale też ich udźwiękowione wersje, czyli voiceboty oraz asystenci głosowi tacy jak Google Asystent, Alexa, Siri czy Bixby.

Chcielibyśmy, aby chatboty potrafiły podjąć się interpretacji całej złożoności ludzkiej komunikacji, jednak nawet bez tego – radzą sobie coraz lepiej. Dzisiaj bot NLP nie tylko rozumienie znaczenie słów. Chatboty potrafią świetnie zinterpretować wypowiedzi, odszyfrować ciągi alfanumeryczne, dane osobowe, rozpoznawać imiona i nazwiska, nazwy własne, adresy, numery telefonów. Dodając do tego umiejętność umieszczania dialogu w kontekście, uzyskiwane jest całkiem dobry wynik w odczytywaniu zamiarów (intencji) użytkowników. To bardzo praktyczne zastosowania, które na co dzień są bardzo przydatne w biznesie i różnego typu organizacjach. Chatboty NLP używane są bowiem nie tylko do prowadzenia prostych rozmów, ale też obsługi całych procesów.

Dostawcy usług chatbotowych zazwyczaj posiadają swoje własne NLP. Jego jakość ma duży wpływ na finalną skuteczność chatbota. Jednak nie każdy bot przygotowany na tym samym narzędziu uzyskuje identyczne wyniki. 

**b) https://sunscrapers.com/blog/8-best-python-natural-language-processing-nlp-libraries/**

Technologie oparte na NLP mogą dostarczyć szerokiego wachlarza cennych spostrzeżeń i rozwiązań problemów językowych, z jakimi mogą spotkać się konsumenci podczas interakcji z produktem.

Nie bez powodu giganci technologiczni, tacy jak Google, Amazon czy Facebook, przeznaczają miliony dolarów na tę dziedzinę badań, aby zasilić swoje chatboty, wirtualnych asystentów, silniki rekomendacji i inne rozwiązania oparte na uczeniu maszynowym.

Ponieważ NLP opiera się na zaawansowanych umiejętnościach obliczeniowych, programiści potrzebują najlepszych dostępnych narzędzi, które pomogą w pełni wykorzystać podejścia i algorytmy NLP do tworzenia usług, które mogą obsługiwać języki naturalne.

Obecnie programiści mogą korzystać z gotowych narzędzi, które upraszczają wstępne przetwarzanie tekstu, dzięki czemu mogą skupić się na budowaniu modeli uczenia maszynowego.

Python dostarcza programistom bogatą kolekcję narzędzi i bibliotek NLP, które pozwalają na obsługę dużej liczby zadań związanych z NLP, takich jak klasyfikacja dokumentów, modelowanie tematów, znakowanie części mowy (POS), wektory słów i analiza sentymentu.

Przykłady takich bibliotek:
- Natural Language Toolkit (NLTK): odegrała kluczową rolę w przełomowych badaniach nad NLP; jest podstawową biblioteką wspierającą takie zadania jak klasyfikacja, tagowanie, parsowanie, rozumowanie semantyczne i tokenizacja w Pythonie. Jest to w zasadzie główne narzędzie do przetwarzania języka naturalnego i uczenia maszynowego. 
Biblioteka ta jest dość wszechstronna, ale jest też doiść trudna w użyciu dla NLP w języku Python. NLTK może nie być dość szybki by odpowiedzieć na wymagania szybkiego użycia w produkcji. Krzywa uczenia się jest stroma, ale programiści mogą skorzystać z zasobów NLTK aby dowiedzieć się wuęcej o koncepcjach stojących za zadaniami przetwarzania języka obsługiwanymi przez ten zestaw narzędzi.

- TextBlob: jest podstawą dla programistów chcących wejść w temat NLP w Pythonie przy jednocześnie maksymalnym uzysku podczas pierwszego natknięcia się na NLTK. Zapewnia ona prosty interfejs do nauki wiekszości prostych zadań w NLP jak analiza sentymentów, post-tagging lub ekstrakcja rzeczownika z wyrażenia. Jednak ta biblioteka, podobnie jak NLTK jest zbyt powolna by sprostać wymaganiom użycia produkcyjnego.

- CoreNLP: Największą zaletą biblioteki CoreNLP jest jej szybkość i sprawność w środowiskach product dewelopmentu. Niektóre komponenty CoreNLP mogą być integrowane z NLTK co daje nam finalnie wzrost efektywności.

- Gensim: specjalizuje się w identyfikowaniu podobieństw semantycznych między dwoma dokumentami przy pomocy zestawu narzędzi do modelowania przestrzeni wektorowej oraz modelowania tematycznego.
Może obsługiwać duże bloki tekstowe za pomocą wydajnego strumieniowania danych i algorytmów przyrostowych, co jest więcej niż możemy powiedzieć o innych pakietach, które są ukierunkowane tylko na przetwarzanie wsadowe i w pamięci. Największa jego zaleta to niesamowita optymalizacja wykorzystania pamięci i szybkość przetwarzania. Zostały one osiągnięte z pomocą innej biblioteki Pythona, NumPy.

- spaCy: jest stosunkowo młodą biblioteką, która została zaprojektowana do użytku produkcyjnego. SpaCypaCy oferuje najszybszy parser składniowy dostępny obecnie na rynku. Co więcej, ponieważ zestaw narzędzi napisany jest w Cythonie, jest on również bardzo szybki i wydajny.

- polygot: oferuje szeroki zakres analiz i imponujące pokrycie językowe. Dzięki NumPy, działa również naprawdę szybko. Używanie polyglota jest podobne do spaCy - jest bardzo wydajny i prosty. Biblioteka wyróżnia się z tłumu również tym, że wymaga użycia dedykowanej komendy w linii poleceń poprzez mechanizmy pipeline.

- scikit-learn: Ta poręczna biblioteka NLP udostępnia programistom szeroki zakres algorytmów do budowy modeli uczenia maszynowego. Oferuje wiele funkcji pozwalających na wykorzystanie metody bag-of-words do tworzenia cech w celu rozwiązywania problemów klasyfikacji tekstu. Biblioteka ta nie wykorzystuje jednak sieci neuronowych do wstępnego przetwarzania tekstu. 

- Pattern: Pattern pozwala na tagowanie części mowy, analizę sentymentu, modelowanie przestrzeni wektorowej, SVM, klasteryzację, wyszukiwanie n-gramów i WordNet. Można skorzystać z parsera DOM, crawlera stron internetowych, a także kilku przydatnych API, takich jak Twitter czy Facebook. Mimo to, narzędzie to jest w zasadzie web minerem i może nie wystarczyć do wykonania innych zadań związanych z przetwarzaniem języka naturalnego.

Dzięki rozbudowanemu zestawowi narzędzi i bibliotek Python NLP programiści otrzymują wszelkie wsparcie, jakiego potrzebują podczas budowania niesamowitych narzędzi.
Te 8 bibliotek i wrodzone cechy języka Python sprawiają, że jest on najlepszym wyborem dla każdego projektu, który opiera się na maszynowym rozumieniu ludzkich języków.

**c) https://sloboda-studio.com/blog/how-to-use-nlp-for-building-a-chatbot/**

NLP jest dziedziną informatyki, lingwistyki matematycznej, uczenia maszynowego i sztucznej inteligencji. NLP pomaga chatbotom analizować ludzki język i generować tekst. 

Nasz język jest zjawiskiem wysoce niestrukturalnym, o elastycznych regułach. Jeśli chcemy, aby algorytmy komputerowe rozumiały te dane, powinniśmy przekształcić ludzki język w formę logiczną.

Z pomocą rozumienia języka naturalnego (NLU) i generowania języka naturalnego (NLG) możliwa jest pełna automatyzacja takich procesów jak generowanie raportów finansowych czy analiza statystyk.

Chatboty skryptowe: W przypadku napotkania zadania, które nie zostało zapisane w jego kodzie, bot nie będzie w stanie go wykonać.

Chatboty AI: Są one oparte na NLP. Przetwarzanie języka naturalnego dla chatbota sprawia, że takie boty są bardzo podobne do ludzi. Reagują one na znaczenie całego pytania. Chatbot oparty na AI może uczyć się z każdej interakcji i poszerzać swoją wiedzę. Chatbot oparty na NLP to program komputerowy lub sztuczna inteligencja, która komunikuje się z klientem za pomocą metod tekstowych lub dźwiękowych.

W naszej mowie mamy wiele elementów, które wpływają na zrozumienie chatbota i mogą stać się wyzwaniem w przetwarzaniu języka naturalnego np. Synonimy, homonimy, slang, błędy ortograficzne, skróty, akcenty. 


Konstruując swojego pierwszego chatbota, powinniśmy przejść przez następujące etapy: 

- Określić główne cechy przyszłego chatbota 
- zbadać dostępność gotowych narzędzi, bibliotek
- zbudować chatbota i porządnie go przetestować

**d) https://www.analyticsvidhya.com/blog/2021/07/build-a-simple-chatbot-using-python-and-nltk/**

Istnieją dwa typy czatbotów:

1) Chatboty oparte na regułach - Jak sama nazwa wskazuje, istnieją pewne reguły, na których chatbot działa. Podobnie jak w przypadku modelu uczenia maszynowego, szkolimy chatboty na intencjach użytkowników i odpowiednich odpowiedziach, a na podstawie tych informacji chatbot identyfikuje nowego użytkownika i odpowiada na nie.

2) Samouczące się chatboty - Samouczące się boty są wysoce wydajne, ponieważ są w stanie złapać i zidentyfikować intencje użytkownika na własną rękę. Są one zbudowane przy użyciu zaawansowanych narzędzi i technik uczenia maszynowego, głębokiego uczenia i NLP. 

Chatboty są najlepszym zastosowaniem przetwarzania języka naturalnego i dziś są łatwe do stworzenia i zintegrowania z różnymi mediami społecznościowymi i stronami internetowymi. 

**e) https://machinelearningmastery.com/natural-language-processing/**

Przetwarzanie języka naturalnego, lub w skrócie NLP, jest szeroko zdefiniowane jako automatyczna manipulacja językiem naturalnym, takim jak mowa i tekst, przez oprogramowanie.

Badanie przetwarzania języka naturalnego istnieje od ponad 50 lat i wywodzi się z dziedziny lingwistyki wraz ze wzrostem liczby komputerów.

Język naturalny odnosi się do sposobu, w jaki my, ludzie, komunikujemy się między sobą. Jest to możliwe zarówno poprzez mowę jak i tekst.

Biorąc pod uwagę znaczenie tego typu danych, musimy mieć metody, aby zrozumieć dogłębnie język naturalny. 

Lingwistyka obliczeniowa stała się również znana pod nazwą Natural Language Processing (NLP), aby odzwierciedlić bardziej inżynierskie lub empiryczne podejście metod statystycznych.

Dominacja statystyki w tej dziedzinie prowadzi również często do określania NLP jako Statistical Natural Language Processing (Statystyczne przetwarzanie języka naturalnego), być może w celu zdystansowania jej od klasycznych metod lingwistyki obliczeniowej.

4. Adam
							
**a) https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32**

NLP jest technologią używaną do nauczania komputerów ludzkiego języka. Jako część AI, NLP skupia się na interacjach
człowiek <-> komputer i pozwala maszynom rozumieć język naturalny. Jest to technologia zdecydowanie daleka od ideału
jednak prężnie rozwijająca się i już dziś jest nierozłącznym elementem naszego życia:

- Google Translator,

- Przetwarzanie słów w oprogramowaniu takim jak MS Word,

- Grammarly,

- Interkatywni asystenci używani w Call Center, tzw IVR,

- Asystenci personalni, tj: OK Google, Siri, Cortana, Alexa.

Wszystkie te technologie wykorzystują NLP.

Trudności przy tworzeniu są nieoczywiste i abstrakcyjne - jak np. nauczyć komputer ironii?

**b) https://ai.pwn.pl/blog/nlp-co-to-jest-i-do-czego-sie-przydaje**

**Czym jest język naturalny?** Język naturalny to język, którym komunikują się ludzie między sobą (np. język polski,
język angielski itp.) – w przeciwieństwie, przykładowo, do języków programowania.

**Czym jest przetwarzanie języka naturalnego?** NLP (ang. Natural Language Processing), czyli po polsku: przetwarzanie
języka naturalnego, to operacje wykonywane przez komputer na wypowiedziach języka naturalnego. Operacje te mogą mieć na
celu zrozumienie wypowiedzi, przełożenie jej na inny język czy też wygenerowanie nowej wypowiedzi.

**Gdzie stosuje się NLP?** Przetwarzanie języka naturalnego stosowane jest w coraz szerszej palecie systemów
komputerowych, realizujących takie zadania jak:

- Dialog między człowiekiem a komputerem - Chatboty to systemy, które “rozmawiają” z użytkownikiem w języku naturalnym,
  starając się skierować jego zainteresowanie na określoną tematykę – najczęściej związaną z działalnością firmy
  udostępniającej tego typu rozwiązanie. Sposób działania chatbota jest z reguły dość prosty – system szuka w
  wypowiedziach użytkownika wyrazów kluczowych, a następnie kieruje go do stron internetowych z nimi związanych.

- Wydobywanie wiedzy z tekstu - Proces pozyskiwania dokumentów zawierających informacje związane ze sformułowanym
  pytaniem nazywamy wydobywaniem informacji (ang. information retrieval). Na przykład w odpowiedzi na prośbę o
  informacje dotyczące aktorów odgrywających rolę agenta Jamesa Bonda znajdujące się w zbiorze recenzji filmowych
  uzyskamy zestaw tych wszystkich recenzji, w których szukana informacja się pojawia. Jeśli natomiast chcemy pozyskać
  informacje podane w określonym formacie, stosujemy proces nazywany ekstrakcją informacji (ang. information extraction)
  . Na przykład w odpowiedzi na prośbę o informacje dotyczące aktorów odgrywających rolę agenta Jamesa Bonda możemy
  uzyskać tabelę zawierającą nazwiska stosownych aktorów oraz tytuły filmów, w których zagrali.

- Analiza znaczenia tekstów - Analiza wydźwięku (ang. sentiment analysis) to rodzaj nadzorowanej klasyfikacji tekstu, w
  ramach której kategoryzuje się opinię wyrażoną w jego treści. Najczęściej stosuje się w tym przypadku podział na trzy
  klasy: pozytywną, negatywną oraz neutralną. W niektórych zastosowaniach analizę wydźwięku odnosi się do określonych
  aspektów wypowiedzi. Na przykład w przypadku recenzji hotelu można skategoryzować opinię zawartą w analizowanym
  tekście ze względu takie aspekty jak: cena, lokalizacja, poziom obsługi czy jakość wyżywienia.

- Poprawa błędów językowych - Korekta pisowni to zadanie, w przypadku którego system informatyczny pełni rolę doradczą –
  wskazuje on potencjalne błędy w tekście, pozostawiając użytkownikowi ostateczną decyzję co do ich poprawy.

**c) https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP**

Mianem NLP określa się program komputerowy umożliwiający maszynom zrozumienie i przetworzenie języka naturalnego.
Niezależnie, czy program przetwrza język w formie mówionej, czy pisanej - do tego zadania wykorzystywana jest sztuczna
inteligencja.

Dlaczego NLP jest tak ważne? NLP ułatwia życie w takich sytuacjach jak konieczność przetrzwania ogromnych ilości tekstu
trzymanych w bazach danych wielko-tysięcznych korporacji. Procesowanie języka naturalnego jest też przydatne w
sytuacjach gdzie kluczowe jest SLA - pomga zautomatyzować i przyspieszyć niektóre procesy.

Głównym profitem z używania NLP jest generalne ułatwienie komunikacji z komputerami. Poza aplikacjami, które wymieniłem
w poprzednich punktach, dziś możemy np. z poziomu asystenta głosowego w telefonie kazać Roombie popsrzątać mieszkanie.

**
d) https://ichi.pro/pl/przewodnik-dla-poczatkujacych-po-przetwarzaniu-jezyka-naturalnego-nlp-w-pythonie-analiza-sentymentu-23879425113355**

Mimo ogólnego skomplikowania danego zagadnienia, podstawowe i działające NLP można stworzyć w kilku prostych krokach:

1. Instalacja bibliotek.

Można oczywiście wykorzystywać różne biblioteki, jednak tymi najpopularniejszymi i standardowymi są:

PyTorch, NLTK, scikit-learn, spaCy

2. Instalacja modelu.

Import zależności

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

Import
torch
```

Dla ułatwienia wykorzystajmy gotowe i wcześniej przygotowane dane
**https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment**

Stworzenie tokenizatora

```python
tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
```

Wczytanie tych danych wyglądałoby następująco:

```python
model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
```

Model na podstawie zadanego tekstu (opinii) zwróci ocenę w skali 1 - 5.

A więc sprawdźmy to:

```python
tokens = tokenizer.encode("this was the best class ever. i wish i could take this class again", return_tensors="pt")
result = model(tokens)
print(result.logits)
print(int(torch.argmax(result.logits)) + 1)
```

> 5

Na pierwszy rzut oka zrozumienie, co się dzieje z wynikami modelu, może być zniechęcające. Jednak w pewnym kontekście
nie jest to trudne do rozszyfrowania. Model informuje nas, że wprowadzony przez nas tekst ma bardzo wysoki sentyment,
stąd wynik 5

**e) https://miroslawmamczur.pl/czym-jest-przetwarzanie-jezyka-naturalnego-nlp-i-jak-zaczac/**

NLP - co to jest?

Natural language processing dziedzina sztucznej inteligencji (AI), która sprawia, że język ludzki jest zrozumiały dla
maszyn

NLP łączy lingwistykę, informatykę i matematykę do badania zasad i struktury języka, co dalej umożliwia stworzenie
systemów zdolnych do zrozumienia i analizowania tekstu lub mowy.

Gdzie stosuje się NLP? Najpopularniejsze przykłady:

- Wyszukiwarki internetowe (Google, Bing)
- Asystenci internetowi (OK Googlem, Siri, Alexa, Cortana)
- Tłumacze (Google Translator)

Trudności przy tworzeniu NLP

Największą trudnością przy tworzeniu NLP jest sama natura języka naturalnego - bardzo trudno jest wytłumaczyć
komputerowi czym jest sarkazm. Problemem jest też niejednoznaczność i nieprecyzyjność języka, np. słowo "zamek" może
oznaczać zamek w bluze, budynek lub mechanizm w drzwiach.

Jak działa przetwarzanie języka naturalnego?

Z wykorzystaniem algorytmów musimy zidentyfikować i wyodrębnić reguły jezyka naturalnego w taki sposób aby stał sie on
zrozumiały dla komputerów.

W pierwszym kroku należy oczyścić i uprościć dane, dzieki temu ich zrozumienie będzie łatwiejsze.

Drugi krok to zamiana słów na liczby - tak aby było to zrozumiałe dla komputerów (tzw. embeddings).

Następnie ma miejsce interpretacja słów za pomocą wektorów.

Techniki NLP:

- Analiza składni - składnia odnosi się do ułożenia słów w zdaniu w taki sposób, aby miały sens gramatyczny. W NLP
  analiza składniowa służy do oceny zgodności języka naturalnego z regułami gramatycznymi. Algorytmy komputerowe służą
  do stosowania reguł gramatycznych do grupy słów i wyprowadzania z nich znaczenia.
- Analiza semantyki - semantyka odnosi się do znaczenia, jakie przekazuje tekst. Analiza semantyczna jest jednym z
  trudnych aspektów przetwarzania języka naturalnego, który nie został jeszcze w pełni rozwiązany. Polega na
  zastosowaniu algorytmów komputerowych w celu zrozumienia znaczenia i interpretacji słów oraz struktury zdań


Zadania związane z procesowaniem tekstu:

- OCR (Optical Character Recognition),
- Rozpoznawanie tematów (topic modeling),
- Automatyczne podsumowanie (automatioc summarization),
- Analiza dokumentów (document analysis),
- Korekcja błędów gramatycznych (Grammatical error correction),
- Tłumaczenie maszynowe (machine translation),
- Rozumienie języka naturalnego (Natural Language Understanding).


